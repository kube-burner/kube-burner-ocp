{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenShift Wrapper","text":"<p>This plugin is a very opinionated OpenShift wrapper designed to simplify the execution of different workloads in this Kubernetes distribution.</p> <p>Executed with <code>kube-burner-ocp</code>, it looks like:</p> <pre><code>$ kube-burner-ocp --help\nkube-burner plugin designed to be used with OpenShift clusters as a quick way to run well-known workloads\n\nUsage:\n  kube-burner-ocp [command]\n\nAvailable Commands:\n  cluster-density-ms         Runs cluster-density-ms workload\n  cluster-density-v2         Runs cluster-density-v2 workload\n  cluster-health             Checks for ocp cluster health\n  completion                 Generate the autocompletion script for the specified shell\n  crd-scale                  Runs crd-scale workload\n  dv-clone                   Runs dv-clone workload\n  egressip                   Runs egressip workload\n  help                       Help about any command\n  index                      Runs index sub-command\n  init                       Runs custom workload\n  kueue-operator-pods        Runs kueue-operator-pods workload\n  kueue-operator-jobs        Runs kueue-operator-jobs workload\n  kueue-operator-jobs-shared Runs kueue-operator-jobs-shared workload\n  network-policy             Runs network-policy workload\n  node-density               Runs node-density workload\n  node-density-cni           Runs node-density-cni workload\n  node-density-heavy         Runs node-density-heavy workload\n  olm                        Runs olm workload\n  pvc-density                Runs pvc-density workload\n  rds-core                   Runs rds-core workload\n  udn-bgp                    Runs udn-bgp workload\n  udn-density-pods           Runs node-density-udn workload\n  version                    Print the version number of kube-burner\n  virt-capacity-benchmark    Runs capacity-benchmark workload\n  virt-clone                 Runs virt-clone workload\n  virt-density               Runs virt-density workload\n  virt-ephemeral-restart     Runs virt-ephemeral-restart workload\n  virt-migration             Runs virt-migration workload\n  virt-udn-density           Runs virt-density-udn workload\n  web-burner-cluster-density Runs web-burner-cluster-density workload\n  web-burner-init            Runs web-burner-init workload\n  web-burner-node-density    Runs web-burner-node-density workload\n  whereabouts                Runs whereabouts workload\n\nFlags:\n      --alerting                  Enable alerting (default true)\n      --burst int                 Burst (default 20)\n      --check-health              Check cluster health before job (default true)\n      --enable-file-logging       Enable file logging (default true)\n      --es-index string           Elastic Search index\n      --es-server string          Elastic Search endpoint\n      --extract                   Extract workload in the current directory\n      --gc                        Garbage collect created resources (default true)\n      --gc-metrics                Collect metrics during garbage collection\n  -h, --help                      help for kube-burner-ocp\n      --local-indexing            Enable local indexing\n      --log-level string          Allowed values: debug, info, warn, error, fatal (default \"info\")\n      --metrics-endpoint string   YAML file with a list of metric endpoints, overrides the es-server and es-index flags\n      --profile-type string       Metrics profile to use, supported options are: regular, reporting or both (default \"both\")\n      --qps int                   QPS (default 20)\n      --timeout duration          Benchmark timeout (default 4h0m0s)\n      --user-metadata string      User provided metadata file, in YAML format\n      --uuid string               Benchmark UUID (default \"e5d2e34e-724d-4ba9-9eac-0379839d2e0a\")\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available here</p>"},{"location":"#usage","title":"Usage","text":"<p>Some of the benefits the OCP wrapper provides are:</p> <ul> <li>Simplified execution of the supported workloads. (Only some flags are required)</li> <li>Adds OpenShift metadata to generated jobSummary and a small subset of metadata fields to the remaining metrics.</li> <li>Prevents modifying configuration files to tweak some of the parameters of the workloads.</li> <li>Discovers the Prometheus URL and authentication token, so the user does not have to perform those operations before using them.</li> <li>Workloads configuration is directly embedded in the binary.</li> </ul> <p>Running node-density with 100 pods per node</p> <pre><code>kube-burner-ocp node-density --pods-per-node=100\n</code></pre> <p>With the command above, the wrapper will calculate the required number of pods to deploy across all worker nodes of the cluster.</p>"},{"location":"#multiple-endpoints-support","title":"Multiple endpoints support","text":"<p>The flag <code>--metrics-endpoint</code> can be used to interact with multiple Prometheus endpoints For example:</p> <pre><code>kube-burner-ocp cluster-density-v2 --iterations=1 --churn-duration=2m0s --churn-cycles=2 --es-index kube-burner --es-server https://www.esurl.com:443 --metrics-endpoint metrics-endpoints.yaml\n</code></pre>"},{"location":"#metrics-endpointsyaml","title":"metrics-endpoints.yaml","text":"<pre><code>- endpoint: prometheus-k8s-openshift-monitoring.apps.rook.devshift.org\n  metrics:\n    - metrics.yml\n  alerts:\n    - alerts.yml\n  indexer:\n      esServers: [\"{{.ES_SERVER}}\"]\n      insecureSkipVerify: true\n      defaultIndex: {{.ES_INDEX}}\n      type: opensearch\n- endpoint: https://prometheus-k8s-openshift-monitoring.apps.rook.devshift.org\n  token: {{ .TOKEN }}\n  metrics:\n    - metrics.yml\n  indexer:\n      esServers: [\"{{.ES_SERVER}}\"]\n      insecureSkipVerify: true\n      defaultIndex: {{.ES_INDEX}}\n      type: opensearch\n</code></pre> <p><code>.TOKEN</code> can be captured by running <code>TOKEN=$(oc create token -n openshift-monitoring prometheus-k8s)</code></p> <p>Note</p> <p>Avoid passing absolute path of the file with --metrics-endpoint option</p> <p>Metric profile names specified against <code>metrics</code> key should be unique and shouldn't overlap with the existing ones. A metric profile will be looked up in this directory config first for the sake of simplicity and if it doesn't exist, will fallback to our specified path. So in order for our own metric profile to get picked up, we will need to specify its absolute path or name differently whenever there is an overlap with the existing ones.</p>"},{"location":"#cluster-density-workloads","title":"Cluster density workloads","text":"<p>This workload family is a control-plane density focused workload that that creates different objects across the cluster. There are 2 different variants cluster-density-v2 and cluster-density-ms.</p> <p>Each iteration of these create a new namespace, the three support similar configuration flags. Check them out from the subcommand help.</p> <p>Info</p> <p>Workload churning of 1h is enabled by default in the <code>cluster-density</code> workloads; you can disable it by passing <code>--churn=false</code> to the workload subcommand.</p>"},{"location":"#cluster-density-v2","title":"cluster-density-v2","text":"<p>Each iteration creates the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>1 build. The OCP internal container registry must be set-up previously because the resulting container image will be pushed there.</li> <li>3 deployments with two pod 2 replicas (nginx) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 deployments with two pod 2 replicas (curl) mounting 4 Secrets, 4 config maps and 1 downward API volume each. These pods have configured a readiness probe that makes a request to one of the services and one of the routes created by this workload every 10 seconds.</li> <li>5 services, each one pointing to the TCP/8080 port of one of the nginx deployments.</li> <li>2 edge routes pointing to the to first and second services respectively.</li> <li>10 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> <li>3 network policies:<ul> <li>deny-all traffic</li> <li>allow traffic from client/nginx pods to server/nginx pods</li> <li>allow traffic from openshift-ingress namespace (where routers are deployed by default) to the namespace</li> </ul> </li> </ul>"},{"location":"#cluster-density-ms","title":"cluster-density-ms","text":"<p>Lightest version of this workload family, each iteration the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>4 deployments with two pod replicas (pause) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 services, each one pointing to the TCP/8080 and TCP/8443 ports of the first and second deployment respectively.</li> <li>1 edge route pointing to the to first service.</li> <li>20 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> </ul>"},{"location":"#node-density-workloads","title":"Node density workloads","text":"<p>The workloads of this family create a single namespace with a set of pods, deployments, and services depending on the workload.</p>"},{"location":"#node-density","title":"node-density","text":"<p>This workload is meant to fill with pause pods all the worker nodes from the cluster. It can be customized with the following flags. This workload is usually used to measure the Pod's ready latency KPI.</p>"},{"location":"#node-density-cni","title":"node-density-cni","text":"<p>It creates two deployments, a client/curl and a server/nxing, and 1 service backed by the previous server pods. The client application has configured an startup probe that makes requests to the previous service every second with a timeout of 600s.</p> <p>Note: This workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"#node-density-heavy","title":"node-density-heavy","text":"<p>Creates two deployments, a postgresql database, and a simple client that performs periodic insert queries (configured through liveness and readiness probes) on the previous database and a service that is used by the client to reach the database.</p> <p>Note: this workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"#udn-density-l3-pods","title":"udn-density-l3-pods","text":"<p>For User-Defined Network (UDN) L3 segmentation testing. It creates two deployments, a client/curl and a server/nxing.</p>"},{"location":"#network-policy-workloads","title":"Network Policy workloads","text":"<p>Network policy scale testing tooling involved  2 components: 1. Template to include all network policy configuration options 2. Latency measurement through connection testing</p> <p>A network policy defines the rules for ingress and egress traffic between pods in  local and remote namespaces. These remote namespace addresses can be configured using a combination of namespace and pod selectors, CIDRs, ports, and port ranges. Given that network policies offer a wide variety of configuration options, we developed a unified template that incorporates all these configuration parameters. Users can specify the desired count for each option.</p> <pre><code>spec:\n  podSelector:\n    matchExpressions:\n    - key: num\n      operator: In\n      values:\n      - \"1\"\n      - \"2\"\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchExpressions:\n        - key: kubernetes.io/metadata.name\n          operator: In\n          values:\n          - network-policy-perf-13\n          - network-policy-perf-14\n      podSelector:\n       matchExpressions:\n       - key: num\n         operator: In\n         values:\n         - \"1\"\n         - \"2\"\n    ports:\n    - port: 8080\n      protocol: TCP\n</code></pre>"},{"location":"#scale-testing-and-unique-acl-flows","title":"Scale Testing and Unique ACL Flows","text":"<p>In our scale tests, we aim to create between 10 to 100 network policies within a single namespace. The primary focus is on preventing duplicate configuration options, which ensures that each network policy generates unique Access Control List (ACL) flows. To achieve this, we carefully designed our templating approach based on the following considerations:</p> <p>Round-Robin Assignment: We use a round-robin strategy to distribute 1. remote namespaces among ingress and egress rules across kube burner job iterations 2. remote namespaces among ingress and egress rules in the same kube burner job iteration</p> <p>This ensures that we don\u2019t overuse the same remote namespaces in a single iteration or among multiple iterations. For instance, if namespace-1 uses namespace-2 and namespace-3 as its remote namespaces, then namespace-2 will start using namespace-4 and namespace-5 as remote namespaces in the next iteration.</p> <p>Unique Namespace and Pod Combinations: To avoid redundant flows, the templating system generates unique combinations of remote namespaces and pods for each network policy. Initially, we iterate through the list of remote namespaces, and once all remote namespaces are exhausted, we move on to iterate through the remote pods. This method ensures that every network policy within a namespace is assigned a distinct combination of remote namespaces and remote pods, avoiding duplicate pairs.</p> <p>Templating Logic Our templating logic is implemented as follows: <pre><code>// Iterate over the list of namespaces to configure network policies.\nfor namespace := namespaces {\n\n  // Each network policy uses a combination of a remote namespace and a remote pod to allow traffic.\n  for networkPolicy := networkPolicies {\n\n    /*\n    Iterate through the list of remote pods. Once all remote namespaces are exhausted,\n    continue iterating through the remote pods to ensure unique namespace/pod combinations.\n    */\n    for i, remotePod := range remotePods {\n        // Stop when we reach the maximum number of remote pods allowed.\n        if i == num_remote_pods {\n            break\n        }\n\n        // Iterate through the list of remote namespaces to pair with the remote pod.\n        for idx, remoteNamespace := range remoteNamespaces {\n            // Combine the remote namespace and pod into a unique pair for ACL configuration.\n            combine := fmt.Sprintf(\"%s:%s\", remoteNamespace, remotePod)\n\n            // Stop iterating once we\u2019ve exhausted the allowed number of remote namespaces.\n            if idx == num_remote_namespace {\n                break\n            }\n        }\n    }\n  }\n}\n</code></pre></p> <p>CIDRs and Port Ranges We apply the same round-robin and unique combination logic to CIDRs and port ranges, ensuring that these options are not reused in network policies within the same namespace.</p> <p>Connection Testing Support kube-burner measures network policy latency through connection testing. Currently, all pods are configured to listen on port 8080. As a result, client pods will send requests to port 8080 during testing.</p> <p>Note: Egress rules should not be enabled for network policy latency measurement connection testing.</p>"},{"location":"#egressip-workloads","title":"EgressIP workloads","text":"<p>This workload creates an egress IP for the client pods. SDN (OVN) will use egress IP for the traffic from client pods to external server instead of default node IP.</p> <p>Each iteration creates the following objects in each of the created namespaces:</p> <ul> <li>1 deployment with the configured number of client pod replicas. Client pod runs the quay.io/cloud-bulldozer/eipvalidator app which periodically sends http request to the configured \"EXT_SERVER_HOST\" server at an \"DELAY_BETWEEN_REQ_SEC\" interval with a request timeout of \"REQ_TIMEOUT_SEC\" seconds. Client pod then validates if the body of the response has configured \"EGRESS_IPS\". Once the client pod starts running and after receiving first successful response with configured \"EGRESS_IPS\", it sets \"eip_startup_latency_total\" prometheus metric.</li> <li>1 EgressIP object. EgressIP object is cluster scoped. EgressIP object will have number of egress IP addresses which user specified through \"addresses-per-iteration\" cli option. kube-burner generates these addresses for the egressIP object from the egress IP list provided by kube-burner-ocp. OVN applies egressIPs to the pods in the current job iteration because of \"namespaceSelector\" and \"podSelector\" fields in the egressIP object.</li> </ul> <p>Note: User has to manually create the external server or use the e2e-benchmarking(https://github.com/cloud-bulldozer/e2e-benchmarking/tree/master/workloads/kube-burner-ocp-wrapper#egressip) which deploys external server and runs the workload with required configuration.</p> <p>Running 1 iteration with 1 egress IP address per iteration (or egressIP object).</p> <pre><code>kube-burner-ocp egressip --addresses-per-iteration=1 --iterations=1 --external-server-ip=10.0.34.43\n</code></pre> <p>With the command above, each namespace has one pod with a dedicated egress IP. OVN will use this dedicated egress IP for the http requests from client pod's to 10.0.34.43.</p>"},{"location":"#web-burner-workloads","title":"Web-burner workloads","text":"<p>This workload is meant to emulate some telco specific workloads. Before running web-burner-node-density or web-burner-cluster-density load the environment with web-burner-init first (without the garbage collection flag: <code>--gc=false</code>).</p> <p>Pre-requisites:</p> <ul> <li>At least two worker nodes</li> <li>At least one of the worker nodes must have the <code>node-role.kubernetes.io/worker-spk</code> label</li> </ul>"},{"location":"#web-burner-init","title":"web-burner-init","text":"<ul> <li>35 (macvlan/sriov) networks for 35 lb namespace</li> <li>35 lb-ns</li> <li>1 frr config map, 4 emulated lb pods on each namespace</li> <li>35 app-ns<ul> <li>1 emulated lb pod on each namespace for bfd session</li> </ul> </li> </ul>"},{"location":"#web-burner-node-density","title":"web-burner-node-density","text":"<ul> <li>35 app-ns</li> <li>3 app pods and services on each namespace</li> <li>35 normal-ns<ul> <li>1 service with 60 normal pod endpoints on each namespace</li> </ul> </li> </ul>"},{"location":"#web-burner-cluster-density","title":"web-burner-cluster-density","text":"<ul> <li>20 normal-ns<ul> <li>30 configmaps, 38 secrets, 38 normal pods and services, 5 deployments with 2 replica pods on each namespace</li> </ul> </li> <li>35 served-ns</li> <li>3 app pods on each namespace</li> <li>2 app-served-ns<ul> <li>1 service(15 ports) with 84 pod endpoints, 1 service(15 ports) with 56 pod endpoints, 1 service(15 ports) with 25 pod endpoints</li> <li>3 service(15 ports each) with 24 pod endpoints, 3 service(15 ports each) with 14 pod endpoints</li> <li>6 service(15 ports each) with 12 pod endpoints, 6 service(15 ports each) with 10 pod endpoints, 6 service(15 ports each) with 9 pod endpoints</li> <li>12 service(15 ports each) with 8 pod endpoints, 12 service(15 ports each) with 6 pod endpoints, 12 service(15 ports each) with 5 pod endpoints</li> <li>29 service(15 ports each) with 4 pod endpoints, 29 service(15 ports each) with 6 pod endpoints</li> </ul> </li> </ul>"},{"location":"#core-rds-workloads","title":"Core RDS workloads","text":"<p>The telco core reference design specification (RDS) describes OpenShift Container Platform clusters running on commodity hardware that can support large scale telco applications including control plane and some centralized data plane functions. It captures the recommended, tested, and supported configurations to get reliable and repeatable performance for clusters running the telco core profile.</p> <p>Pre-requisites:  - A PerformanceProfile with isolated and reserved cores, 1G hugepages and and <code>topologyPolicy=single-numa-node</code>. Hugepages should be allocated in the first NUMA node (the one that would be used by DPDK deployments):      <pre><code> hugepages:\n defaultHugepagesSize: 1G\n pages:\n - count: 160\n   node: 0\n   size: 1G\n - count: 6\n   node: 1\n   size: 1G\n</code></pre>  - MetalLB operator limiting speaker pods to specific nodes (approx. 10%, 12 in the case of 120 node iterations with the corresponding worker-metallb label):      <pre><code>apiVersion: metallb.io/v1beta1\nkind: MetalLB\nmetadata:\n  name: metallb\n  namespace: metallb-system\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/worker-metallb: \"\"\n  speakerTolerations:\n  - key: \"Example\"\n    operator: \"Exists\"\n    effect: \"NoExecute\"\n</code></pre>  - SRIOV operator with its corresponding SriovNetworkNodePolicy  - Some nodes (i.e.: 25% of them) with the worker-dpdk label to host the DPDK pods, i.e.:      <pre><code>$ kubectl label node worker1 node-role.kubernetes.io/worker-dpdk=\n</code></pre></p> <p>Object count: | Iterations / nodes / namespaces   | 1    | 120                                 | | --------------------------------- | ---- | ----------------------------------- | | configmaps                        | 30   | 3600                                | | deployments_best_effort           | 25   | 3000                                | | deployments_dpdk                  | 2    | 240 (assuming 24 worker-dpdk nodes) | | endpoints (210x service)          | 4200 | 504000                              | | endpoints lb (90 x service)       | 90   | 10800                               | | networkPolicy                     | 3    | 360                                 | | namespaces                        | 1    | 120                                 | | pods_best_effort (2 x deployment) | 50   | 6000                                | | pods_dpdk (1 x deployment)        | 2    | 240 (assuming 24 worker-dpdk nodes) | | route                             | 2    | 240                                 | | services                          | 20   | 2400                                | | services (lb)                     | 1    | 120                                 | | secrets                           | 42   | 5040                                |</p> <p>Input parameters specific to the workload: | Parameter           | Description                                                                                      | Default value | | ------------------- | ------------------------------------------------------------------------------------------------ | ------------- | | dpdk-cores          | Number of cores assigned for each DPDK pod (should fill all the isolated cores of one NUMA node) | 2             | | performance-profile | Name of the performance profile implemented on the cluster                                       | default       |</p>"},{"location":"#virt-workloads","title":"Virt Workloads","text":"<p>This workload family is a focused on Virtualization creating different objects across the cluster.</p> <p>The different variants are: - virt-density - virt-udn-density - virt-capacity-benchmark. - virt-clone - virt-ephemeral-restart - virt-migration</p>"},{"location":"#virt-density","title":"Virt Density","text":"<p>Similar to node-density, fills with VirtualMachines the worker nodes of the cluster (kubevirt/OpenShift Virtualization is required to run this workload). Meant to detect issues derived from spinning up high amounts VMs in a short amount of time and to track runningthe latencies of the different VM bootstrap stages.</p>"},{"location":"#virt-density-udn","title":"Virt Density Udn","text":"<p>Similar to udn-density-pods scenario. Creates VMs, one Nginx server and several clients (the number depends on the <code>vms-per-node</code> variable) reaching it, on the same UDN per iteration. Each UDN-namespace has the same number of VMs, the number of clients deployed per UDN is computed as following:  <code>Nb of client per UDN = (Nb of worker * vms-per-node / Nb of UDN) -1 //-1  because the server is always deployed.</code> This scenario is meant to test how many UDNs can be deployed in parallel and how it scales. It requires a version of OCP higher than 4.18, otherwise, UDN feature is not available.</p>"},{"location":"#virt-capacity-benchmark","title":"Virt Capacity Benchmark","text":"<p>Test the capacity of Virtual Machines and Volumes supported by the cluster and a specific storage class.</p>"},{"location":"#environment-requirements","title":"Environment Requirements","text":"<p>In order to verify that the <code>VirtualMachine</code> completed their boot and that volume resize propagated successfully, the test uses <code>virtctl ssh</code>. Therefore, <code>virtctl</code> must be installed and available in the <code>PATH</code>.</p> <p>See the Temporary SSH Keys for details on the SSH keys used for the test</p>"},{"location":"#test-sequence","title":"Test Sequence","text":"<p>The test runs a workload in a loop without deleting previously created resources. By default it will continue until a failure occurs. Each loop is comprised of the following steps: - Create VMs - Resize the root and data volumes - Restart the VMs - Snapshot the VMs - Migrate the VMs</p>"},{"location":"#tested-storageclass","title":"Tested StorageClass","text":"<p>By default, the test will search for the <code>StorageClass</code> to use:</p> <ol> <li>Use the default <code>StorageClass</code> for Virtualization annotated with <code>storageclass.kubevirt.io/is-default-virt-class</code></li> <li>If does not exist, use general default <code>StorageClass</code> annotated with <code>storageclass.kubernetes.io/is-default-class</code></li> <li>If does not exist, fail the test before starting</li> </ol> <p>To use a different one, use <code>--storage-class</code> to provide a different name.</p> <p>In addition, multiple <code>StorageClasses</code> can be used by passing a comma separated list names. The test will then choose a different <code>StorageClass</code> for each loop in round robin.</p> <p>Please note that regardless to which <code>StorageClass</code> is used, it must: - Support Volume Expansion: <code>allowVolumeExpansion: true</code>. - Have a corresponding <code>VolumeSnapshotClass</code> using the same provisioner</p>"},{"location":"#test-namespace","title":"Test Namespace","text":"<p>All <code>VirtualMachines</code> are created in the same namespace.</p> <p>By default, the namespace is <code>virt-capacity-benchmark</code>. Set it by passing <code>--namespace</code> (or <code>-n</code>)</p>"},{"location":"#test-size-parameters","title":"Test Size Parameters","text":"<p>Users may control the workload sizes by passing the following arguments: - <code>--max-iterations</code> - Maximum number of iterations, or 0 (default) for infinite. In any case, the test will stop upon failure - <code>--vms</code> - Number of VMs for each iteration (default 5) - <code>--data-volume-count</code> - Number of data volumes for each VM (default 9) - <code>--min-vol-size</code> - Set the minimal volume size supported by the storage class - <code>--min-vol-inc-size</code> - Set the minimal volume size increment supported by the storage class</p>"},{"location":"#temporary-ssh-keys","title":"Temporary SSH Keys","text":"<p>The test generated the SSH keys automatically. By default, it stores the pair in a temporary directory. Users may choose the store the key in a specified directory by setting <code>--ssh-key-path</code></p>"},{"location":"#skip-test-parts","title":"Skip test parts","text":"<p>Some storage classes have limitations requiring the test to skip some parts: - <code>--skip-resize-job</code> - Skip volume resize job. Use when e.g. <code>allowVolumeExpansion</code> is <code>false</code> - <code>--skip-migration-job</code> - Skip the migration job. Use when e.g. <code>RWX</code> <code>accessMode</code> is not supported</p>"},{"location":"#cleanup","title":"Cleanup","text":"<p>Since the test is expected to run until failure, it is designed to keep all allocated resources to allow investigating the failure. To cleanup all allocated resources once the test is done set <code>--cleanup</code>. Alternatively, run the test with only the <code>--cleanup-only</code> flag set to cleanup resources from past test runs</p>"},{"location":"#virt-clone","title":"Virt Clone","text":"<p>Test the capacity and performance of starting multiple virtual machines with a root disk as clones of a single volume. This test comes to mimic VDI sequence</p>"},{"location":"#test-sequence_1","title":"Test Sequence","text":"<p>The test runs the following sequence: 1. Create a <code>VirtualMachine</code> in namespace A 2. Stop the <code>VirtualMachine</code> 3. Create a <code>DataVolume</code> in namespace B using the rootdisk of the <code>VirtualMachine</code> as the source 4. If the <code>dataImportCronSourceFormat</code> field of the <code>StorageProfile</code> <code>status</code> is set to <code>snapshot</code>, or <code>--use-snapshot</code> is set to <code>true</code>, create a <code>VolumeSnapshot</code> of the DataVolume 5. Create a <code>DataSource</code>, setting the <code>source</code> field to either the <code>VolumeSnapshot</code> (if was created) or the <code>DataVolume</code> 6. Create <code>VirtualMachines</code> in namespace B based in the <code>DataSource</code></p>"},{"location":"#tested-storageclass_1","title":"Tested StorageClass","text":"<p>By default, the test will use the default <code>StorageClass</code>. To use a different one, use <code>--storage-class</code> to provide a different name.</p> <p>If <code>--use-snapshot</code> is explicitly set to <code>true</code> a corresponding <code>VolumeSnapshotClass</code> using the same provisioner must exist. Otherwise, the test will check the <code>StorageProfile</code> for the <code>StorageClass</code> and act accordingly.</p>"},{"location":"#test-namespace_1","title":"Test Namespace","text":"<p>The test creates <code>VirtualMachines</code> in two namespaces: <code>&lt;baseName&gt;-base</code> and <code>&lt;baseName&gt;-clones</code></p> <p>By default, the <code>baseName</code> is <code>virt-clone</code>. Set it by passing <code>--namespace</code> (or <code>-n</code>)</p>"},{"location":"#test-size-parameters_1","title":"Test Size Parameters","text":"<p>Users may control the workload sizes by passing the following arguments: - <code>--iteration</code> - Number of iterations to run in step 6. Default is 1 - <code>--iteration-clones</code> - Number of <code>VirtualMachines</code> to create in each iteration of step 6. Default is 10</p>"},{"location":"#volume-access-mode","title":"Volume Access Mode","text":"<p>By default, volumes are created with <code>ReadWriteMany</code> access mode as this is the recommended configuration for <code>VirtualMachines</code>. If not supported, the access mode may be changes by setting <code>--access-mode</code>. The supported values are <code>RO</code>, <code>RWO</code> and <code>RWX</code>.</p>"},{"location":"#temporary-ssh-keys_1","title":"Temporary SSH Keys","text":"<p>In order to verify that the VMs actually completed booting, the test generates an SSH key pair. By default, it stores the pair in a temporary directory. Users may choose the store the key in a specified directory by setting <code>--ssh-key-path</code></p>"},{"location":"#virt-ephemeral-restart","title":"Virt Ephemeral Restart","text":"<p>Test the performance of restarting ephemeral <code>VirtalMachine</code>s. Kubernetes native ephemeral volumes use local node storage. As a result, the cannot be used on large scale deployment. Instead, a restart is implemented by stopping the <code>VirtualMachine</code>, deleting the <code>DataVolume</code> backing its root volume and starting it.</p>"},{"location":"#test-sequence_2","title":"Test Sequence","text":"<p>The test runs the following sequence: 1. Create a <code>DataVolume</code> using a container image as the source 2. If the <code>dataImportCronSourceFormat</code> field of the <code>StorageProfile</code> <code>status</code> is set to <code>snapshot</code>, or <code>--use-snapshot</code> is set to <code>true</code>, create a <code>VolumeSnapshot</code> of the DataVolume 3. Create a <code>DataSource</code>, setting the <code>source</code> field to either the <code>VolumeSnapshot</code> (if was created) or the <code>DataVolume</code> 4. Create <code>VirtualMachine</code>s based in the <code>DataSource</code> 5. Stop all <code>VirtualMachine</code>s 6. In batches, delete the <code>DataVolume</code> backing the root disk and start the <code>VirtualMachine</code>s</p>"},{"location":"#tested-storageclass_2","title":"Tested StorageClass","text":"<p>By default, the test will use the default <code>StorageClass</code>. To use a different one, use <code>--storage-class</code> to provide a different name.</p> <p>If <code>--use-snapshot</code> is explicitly set to <code>true</code> a corresponding <code>VolumeSnapshotClass</code> using the same provisioner must exist. Otherwise, the test will check the <code>StorageProfile</code> for the <code>StorageClass</code> and act accordingly.</p>"},{"location":"#test-namespace_2","title":"Test Namespace","text":"<p>All <code>VirtualMachines</code> are created in the same namespace.</p> <p>By default, the namespace is <code>virt-ephemeral-restart</code>. Set it by passing <code>--namespace</code> (or <code>-n</code>)</p>"},{"location":"#test-size-parameters_2","title":"Test Size Parameters","text":"<p>Users may control the workload sizes by passing the following arguments: - <code>--iteration-vms</code> - Number of <code>VirtualMachines</code> to batch in each group in step 6 - <code>--iteration</code> - Number of batches to run in step 6</p> <p>Note</p> <p>The total number of <code>VirtualMachines</code> created is <code>--iteration-vms</code> * <code>--iteration</code></p>"},{"location":"#volume-access-mode_1","title":"Volume Access Mode","text":"<p>By default, volumes are created with <code>ReadWriteMany</code> access mode as this is the recommended configuration for <code>VirtualMachines</code>. If not supported, the access mode may be changes by setting <code>--access-mode</code>. The supported values are <code>RO</code>, <code>RWO</code> and <code>RWX</code>.</p>"},{"location":"#temporary-ssh-keys_2","title":"Temporary SSH Keys","text":"<p>In order to verify that the VMs actually completed booting, the test generates an SSH key pair. By default, it stores the pair in a temporary directory. Users may choose the store the key in a specified directory by setting <code>--ssh-key-path</code></p>"},{"location":"#virt-migration","title":"Virt Migration","text":"<p>Test how the cluster and the storage backend handles mass migration of VMs</p>"},{"location":"#test-sequence_3","title":"Test Sequence","text":"<p>The test runs the following sequence: 1. Create <code>VirtualMachines</code> with a node affinity rule that ties them to a specific worker node 2. Remove the node affinity rule to allow them to migrate 3. Call <code>Migrate</code> on all <code>VirtualMachines</code></p>"},{"location":"#tested-storageclass_3","title":"Tested StorageClass","text":"<p>If not <code>--storage-class</code> is not set, the test will:</p> <ol> <li>Use the default <code>StorageClass</code> for Virtualization annotated with <code>storageclass.kubevirt.io/is-default-virt-class</code></li> <li>If does not exist, use general default <code>StorageClass</code> annotated with <code>storageclass.kubernetes.io/is-default-class</code></li> <li>If does not exist, fail the test before starting</li> </ol>"},{"location":"#test-size-parameters_3","title":"Test Size Parameters","text":"<p>Users may control the workload sizes by passing the following arguments: - <code>--iterations</code> - How many iterations to use when creating <code>VirtualMachines</code> in step 1 - <code>--iteration-vms</code> - How many <code>VirtualMachines</code> to create in each iteration in step 1 - <code>--data-volume-count</code> - Number of data volumes for each VM</p> <p>Note</p> <p>The total number of <code>VirtualMachines</code> created is <code>--iterations</code> * <code>--iteration-vms</code></p>"},{"location":"#addition-virtualmachines-load","title":"Addition <code>VirtualMachines</code> load","text":"<p>The test can create additional <code>VirtualMachines</code> that will not be migrated to simulate load on the other nodes as well. By default, no additional <code>VirtualMachines</code> are created.</p> <p>Set the following arguments to create load <code>VirtualMachines</code>: - <code>--load-iterations</code> - Number of iterations to create load VMs - <code>--load-per-iteration</code> - Number of VMs to create in each load VM iteration</p> <p>Note</p> <p>The total number of load <code>VirtualMachines</code> created is <code>--load-vms-iterations</code> * <code>--iteration-load-vms</code></p>"},{"location":"#limiting-migration-requests-load","title":"Limiting migration requests load","text":"<p>By default, the test will be limited to 20 concurrent migrate request calls. Users can set a different value by passing <code>--migration-qps</code>.</p> <p>Note</p> <p>This parameter limits the concurrent migration requests, not actual concurrent migrations</p>"},{"location":"#initial-worker-node","title":"Initial Worker Node","text":"<p>The worker node on which all VMs are scheduled and migrated from can be set by passing the <code>--worker-node</code> parameter.</p> <p>If not set, the test will randomly choose one</p>"},{"location":"#test-namespace_3","title":"Test Namespace","text":"<p>All <code>VirtualMachines</code> are created in the same namespace.</p> <p>By default, the namespace is <code>virt-migation</code>. Set it by passing <code>--namespace</code> (or <code>-n</code>)</p>"},{"location":"#temporary-ssh-keys_3","title":"Temporary SSH Keys","text":"<p>In order to verify that the VMs actually completed booting, the test generates an SSH key pair. By default, it stores the pair in a temporary directory. Users may choose the store the key in a specified directory by setting <code>--ssh-key-path</code></p>"},{"location":"#datavolume-clone","title":"DataVolume Clone","text":"<p>Test the capacity and performance of creating multiple data volumes that are clones of a single data volume</p>"},{"location":"#test-sequence_4","title":"Test Sequence","text":"<p>The test runs the following sequence: 1. Create a <code>DataVolume</code> based on a container disk image 2. If the <code>dataImportCronSourceFormat</code> field of the <code>StorageProfile</code> <code>status</code> is set to <code>snapshot</code>, or <code>--use-snapshot</code> is set to <code>true</code>, create a <code>VolumeSnapshot</code> of the DataVolume 3. Create a <code>DataSource</code>, setting the <code>source</code> field to either the <code>VolumeSnapshot</code> (if was created) or the <code>DataVolume</code> 4. Create <code>DataVolumes</code> based on the <code>DataSource</code>. The creation process is divided into iterations</p>"},{"location":"#tested-storageclass_4","title":"Tested StorageClass","text":"<p>By default, the test will use the default <code>StorageClass</code>. To use a different one, use <code>--storage-class</code> to provide a different name.</p> <p>If <code>--use-snapshot</code> is explicitly set to <code>true</code> a corresponding <code>VolumeSnapshotClass</code> using the same provisioner must exist. Otherwise, the test will check the <code>StorageProfile</code> for the <code>StorageClass</code> and act accordingly.</p>"},{"location":"#test-namespace_4","title":"Test Namespace","text":"<p>All the resources are created in the same namespace.</p> <p>By default, the namespace is <code>dv-clone</code>. Set it by passing <code>--namespace</code> (or <code>-n</code>)</p>"},{"location":"#test-container-disk-image","title":"Test Container Disk Image","text":"<p>Users may set the container disk image used as a source to the base <code>DataVolume</code> using the <code>--container-disk</code> parameter. When setting <code>--container-disk</code>, make sure that it can fit to the default Volume Size <code>1Gi</code> or set a new size by passing <code>--datavolume-size</code>.</p> <p>If not passed, the test will use  <code>quay.io/yblum/tiny_image:latest</code> for the the container disk image.</p>"},{"location":"#test-size-parameters_4","title":"Test Size Parameters","text":"<p>Users may control the workload sizes by passing the following arguments: - <code>--iteration</code> - Number of iterations to run in step 4 - <code>--iteration-clones</code> - Number of <code>DataVolumes</code> to create in each iteration of step 4</p>"},{"location":"#volume-access-mode_2","title":"Volume Access Mode","text":"<p>By default, volumes are created with <code>ReadWriteMany</code> access mode as this is the recommended configuration for <code>VirtualMachines</code>. If not supported, the access mode may be changes by setting <code>--access-mode</code>. The supported values are <code>RO</code>, <code>RWO</code> and <code>RWX</code>.</p>"},{"location":"#cudn-bgp-workload","title":"CUDN BGP Workload","text":"<p>This workload tests BGP route exchange import and export scenarios for the CUDNs.</p> <p>Assumptions in this workload: 1. Kube burner must be running on a Linux host 2. Kube burner should be running on the host which is not used as a bastion host to deploy OCP cluster.    a) Routes in the CUDN gateway router when we have the same host as the deployment host and kube burner host <pre><code>sh-5.1# ovn-nbctl lr-route-list GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\nIPv4 Routes\nRoute Table &lt;main&gt;:\n              20.0.2.0/24             192.168.0.1 dst-ip rtoe-GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\n                0.0.0.0/0               192.168.0.1 dst-ip rtoe-GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\n</code></pre>    b) Routes in the CUDN gateway router when the deployment host and the kube burner host is different <pre><code>sh-5.1# ovn-nbctl lr-route-list GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\nIPv4 Routes\nRoute Table &lt;main&gt;:\n              20.0.2.0/24             192.168.0.145 dst-ip rtoe-GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\n                0.0.0.0/0               192.168.0.1 dst-ip rtoe-GR_cluster_udn_cudn.0_e34-h14-000-r650.rdu2.scalelab.redhat.com\n</code></pre>    In case of same host i.e scenario a, ping reply will reach the kube burner even if the route \"20.0.2.0/24\" is not added, but using default route \"0.0.0.0/0 192.168.0.1\". Our purpose of the testing is to verify if the external route \"20.0.2.0/24\" is properly added or not in CUDN's gateway router. So we want the ping test to fail if this route is not correctly imported.</p> <ol> <li>An external FRR will be running on the same host where the kube burner is running.    a) Here kube burner is the generator of the external routes. External FRR imports these routes into the OCP cluster through internal FRR and OVN.    b) Also when external FRR routes receive the routes from the OCP cluster, kube burner validates them.</li> <li>External FRR will be created by the user and configured to pair up with OCP cluster's OVN internal FRR routers</li> <li>External FRR additionally configured to advertise the host routes i.e</li> </ol> <pre><code>vtysh\nconfigure terminal\nrouter bgp 64512\nredistribute static\nredistribute connected\nend\nwrite\n</code></pre> <p>Unlike a UDN network, a CUDN network will be cluster scoped and can be used by multiple namespaces. However we restrict this to one namespace by default as our aim here is testing BGP route exchange.</p> <p>This workload defines multiple jobs as per CUDN requirement. Some of the requirements: 1. Namespace with label \"k8s.ovn.org/primary-user-defined-network\" before the CUDN creation 2. OVN should create all the necessary resources for CUDN before a pod is created on it. Currently we don't have a mechanism to detect if the OVN has created all CUDN's resources. So we are using separate jobs for CUDN and pods with jobPause. Workload defines only one pod per CUDN. 3. RouteAdvertiments CRD selecting the CUDN. We use 1:1 RA:CUDN mapping.</p> <p>As we want to measure BGP route exchange latency, this workload skips measurements for all the resources except RouteAdvertisements.</p> <p>This workload has 2 BGP route exchange scenarios 1) route export scenaio 2) route import scenario When a RouteAdvertisement is created, it advertises the selected CUDN's subnet to outside cluster. However, an external route is imported to the CUDN's gateway router only when it is advertised by the routeAdvertisment. Hence RouteAdvertisemnt for CUDN is mandatory for both export and import scenarios.</p> <p>Sequent of events during workload execution 1. Job1 creates namespaces 2. Job2 creates CUDNs 3. Job3 creates Pods 4. When Job4 execution starts, Kube burner calls start measurement.    RouteAdvertisment Latency measurement code then    a. Maintains a list of CUDN subnets and the pod addresses.    b. Starts export scenario       i) Main thread subscribes (through routeCh channel) to kernel's netlink sockets for route monitoring       ii) Starts export workers, which read from the subscribed routeCh channel       iii) Registers an informer for notifying router advertisement resource creation events 5. Kube burner creates  RouteAdvertisments for CUDNs    a. Kubernetes API notify the RouteAdvertisment resource to the listening kube burner    b. OVN using the internal FRR advertises this route to the outside cluster (i.e to external FRR)    c. External FRR adds the routes to the host. Kernel notifies the routes to the kube burner using netlink sockets 6. RouteAdvertisment Latency measurement code (watchers and export worker threads) then    a. Records RouteAdvertisement name and creation timestamp when routeadvertisement resource is detected by the API    b. For each route notified by the kernel, pings the corresponding CUDN's pod and records ping success timestamp 7. Kube burner calls stop measurements. RouteAdvertisment Latency measurement code then    a. Waits for export scenario completion    b. Starts import scenario       i) Main thread creates interfaces, prepares IP addresses and pods to ping (which are needed for worker threads)       ii) Starts import workers       iii) Import workers add IP addresses on the interfaces       iv) Kernel creates the linux route for the added IP address. Then external FRR router exports this to the cluster.       v) OVN imports this route into the CUDN's gateway router.       vi) Import worker pings the CUDN pod using the CUDN pod address as destination address and above added IP address as the source address.       vii) Import worker records the ping success timestamp    c. Waits for import scenario complettion 8. kube burner indexes all the latency measurements</p>"},{"location":"#routeadvertisement-latency-metrics","title":"RouteAdvertisement Latency Metrics","text":"<p>RouteAdvetisements latency is calculated for both import and export scenarios, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: podLatency\n</code></pre> <p>The metrics collected are route advertisement latency timeseries (raLatencyMeasurement) and six documents holding a summary with different route latency quantiles of ping test and netlink route detection latency (raLatencyQuantilesMeasurement).</p> <p>One document, such as the following, is indexed per each internal (through Routeadvertisment CRD) and external route (adding ip address on dummy interface) created by the workload:</p> <pre><code>[\n  {\n    \"timestamp\": \"2025-04-15T08:41:10Z\",\n    \"metricName\": \"raLatencyMeasurement\",\n    \"uuid\": \"2c8a64a8-0409-4d17-8643-c28db8216821\",\n    \"jobName\": \"udn-bgp-route-advertisements\",\n    \"routeAdvertisementName\": \"ra-0\",\n    \"metadata\": {\n      \"ocpMajorVersion\": \"4.19\",\n      \"ocpVersion\": \"4.19.0-ec.3\"\n    },\n    \"scenario\": \"ExportRoutes\",\n    \"latency\": [\n      10031\n    ],\n    \"minReadyLatency\": 10031,\n    \"maxReadyLatency\": 10031,\n    \"readyLatency\": 10031,\n    \"netlinkRouteLatency\": [\n      10026\n    ],\n    \"maxNetlinkRouteLatency\": 10026,\n    \"minNetlinkRouteLatency\": 10026,\n    \"p99NetlinkRouteLatency\": 10026\n  },\n  {\n    \"timestamp\": \"2025-04-15T08:42:20.060393739Z\",\n    \"metricName\": \"raLatencyMeasurement\",\n    \"uuid\": \"2c8a64a8-0409-4d17-8643-c28db8216821\",\n    \"jobName\": \"udn-bgp-route-advertisements\",\n    \"routeAdvertisementName\": \"20.0.1.1/24\",\n    \"metadata\": {\n      \"ocpMajorVersion\": \"4.19\",\n      \"ocpVersion\": \"4.19.0-ec.3\"\n    },\n    \"scenario\": \"ImportRoutes\",\n    \"latency\": [\n      13\n    ],\n    \"minReadyLatency\": 13,\n    \"maxReadyLatency\": 13,\n    \"readyLatency\": 13\n  }\n]\n</code></pre>"},{"location":"#olmv1-benchmark","title":"OLMv1 Benchmark","text":"<p>To evaluate the performance of <code>Operator Lifecycle Manager v1</code> (OLMv1), this benchmark initiates the creation of a series of <code>ClusterCatalog</code> and <code>ClusterExtension</code> custom resources. These resources serve as a representative workload to simulate realistic catalog and operator deployment operations within the cluster. Each <code>ClusterExtension</code> internally triggers the resolution and unpacking of an operator bundle through the configured catalog.</p> <p>During the test execution, the system actively monitors and collects detailed resource usage metrics \u2014 specifically CPU and memory consumption \u2014 for the pods running in both the <code>openshift-catalogd</code> and <code>openshift-operator-controller</code> namespaces. These namespaces are critical components of the OLMv1 infrastructure, responsible for catalog resolution and operator deployment, respectively.</p> <p>In addition to standard resource metrics, the performance test is also capable of gathering pprof profiling data, which provides in-depth runtime insights such as goroutine activity, heap allocations, and CPU profiles. This profiling capability is contingent on the user explicitly enabling pprof endpoints within the OLMv1 components prior to test execution. Once enabled, the test can automatically scrape and archive the profiling data for further analysis, facilitating root cause investigation and performance optimization of the OLMv1 stack.</p>"},{"location":"#environment-requirements_1","title":"Environment Requirements","text":"<p>OCP 4.18(OLMv1 GA) and above</p>"},{"location":"#kueue-operator-workloads","title":"Kueue Operator workloads","text":"<p>The workloads of this family are used to exercise the Kueue Operator by creating pods or jobs depending on the workload. In order to run these jobs, Kueue Operator and Kueue CR should be installed on the target cluster.</p>"},{"location":"#kueue-operator-jobs","title":"kueue-operator-jobs","text":"<p>This workload creates jobs in a single namespace that are handled by a single ClusterQueue with pre-defined CPU, memory and pod quotas. Key measurements are Kueue admission wait time, job start and completion latencies.</p>"},{"location":"#kueue-operator-jobs-shared","title":"kueue-operator-jobs-shared","text":"<p>This workload creates jobs in multiple namespaces that are handled by 10 shared ClusterQueues with pre-defined pod quotas. Key measurements are Kueue admission wait time, job start and completion latencies.</p>"},{"location":"#kueue-operator-pods","title":"kueue-operator-pods","text":"<p>This workload creates pods in a single namespace that are handled by a single ClusterQueue with pre-defined CPU, memory and pod quotas. Key measurements are Kueue admission wait time and pod ready latency.</p>"},{"location":"#custom-workload-bring-your-own-workload","title":"Custom Workload: Bring your own workload","text":"<p>To kickstart kube-burner-ocp with a custom workload, <code>init</code> becomes your go-to command. This command is equipped with flags that enable to seamlessly integrate and run your personalized workloads. Here's a breakdown of the flags accepted by the init command:</p> <pre><code>$ kube-burner-ocp init --help\nRuns custom workload\n\nUsage:\n  kube-burner-ocp init [flags]\n\nFlags:\n    --churn                            Enable churning (default true)\n    --churn-cycles int                 Churn cycles to execute\n    --churn-delay duration             Time to wait between each churn (default 2m0s)\n    --churn-deletion-strategy string   Churn deletion strategy to use (default \"default\")\n    --churn-duration duration          Churn duration (default 5m0s)\n    --churn-percent int                Percentage of job iterations that kube-burner will churn each round (default 10)\n    -c, --config string                    Config file path or url\n    -h, --help                             help for init\n    --iterations int                   Job iterations. Mutually exclusive with '--pods-per-node' (default 1)\n    --iterations-per-namespace int     Iterations per namespace (default 1)\n    --namespaced-iterations            Namespaced iterations (default true)\n    --pods-per-node int                Pods per node. Mutually exclusive with '--iterations' (default 50)\n    --service-latency                  Enable service latency measurement\n</code></pre> <p>Creating a custom workload for kube-burner-ocp is a seamless process, and you have the flexibility to craft it according to your specific needs. Below is a template to guide you through the customization of your workload:</p> <pre><code>---\nindexers:\n  - esServers: [\"{{.ES_SERVER}}\"]\n    insecureSkipVerify: true\n    defaultIndex: {{.ES_INDEX}}\n    type: opensearch\nglobal:\n  gc: {{.GC}}\n  gcMetrics: {{.GC_METRICS}}\n  measurements:\n    - name: &lt;metric_name&gt;\n      thresholds:\n        - &lt;threshold_key&gt;: &lt;threshold_value&gt;\n\njobs:\n  - name: &lt;job_name&gt;\n    namespace: &lt;namespace_name&gt;\n    jobIterations: &lt;number of iterations&gt;\n    qps: {{.QPS}}     # Both QPS and BURST can be specified through the CLI\n    burst: {{.BURST}}\n    namespacedIterations: &lt;bool&gt;\n    podWait: &lt;bool&gt;\n    waitWhenFinished: &lt;bool&gt;\n    preLoadImages: &lt;bool&gt;\n    preLoadPeriod: &lt;preLoadPeriod_in_seconds&gt;\n    namespaceLabels:\n      &lt;namespaceLabels_key&gt;: &lt;namespaceLabels_value&gt;\n    objects:\n\n      - objectTemplate: &lt;template_config&gt;\n        replicas: &lt;replica_int&gt;\n        inputVars:\n          &lt;inputVar1&gt;:&lt;inputVar1_value&gt;\n</code></pre> <p>You can start from scratch or explore pre-built workloads in the /config folder, offering a variety of examples used by kube-burner-ocp. Dive into the details of each section in the template to tailor the workload precisely to your requirements. Experiment, iterate, and discover the optimal configuration for your workload to seamlessly integrate with kube-burner-ocp.</p>"},{"location":"#index","title":"Index","text":"<p>Just like the regular kube-burner, <code>kube-burner-ocp</code> also has an indexing functionality which is exposed as <code>index</code> subcommand.</p> <pre><code>$ kube-burner-ocp index --help\nIf no other indexer is specified, local indexer is used by default\n\nUsage:\n  kube-burner-ocp index [flags]\n\nFlags:\n  -m, --metrics-profile string     Metrics profile file (default \"metrics.yml\")\n      --metrics-directory string   Directory to dump the metrics files in, when using default local indexing (default \"collected-metrics\")\n  -s, --step duration              Prometheus step size (default 30s)\n      --start int                  Epoch start time\n      --end int                    Epoch end time\n  -j, --job-name string            Indexing job name (default \"kube-burner-ocp-indexing\")\n      --user-metadata string       User provided metadata file, in YAML format\n  -h, --help                       help for index\n</code></pre>"},{"location":"#metrics-profile-type","title":"Metrics-profile type","text":"<p>By specifying <code>--profile-type</code>, kube-burner can use two different metrics profiles when scraping metrics from prometheus. By default is configured with <code>both</code>, meaning that it will use the regular metrics profiles bound to the workload in question and the reporting metrics profile.</p> <p>When using the regular profiles (metrics-aggregated or metrics), kube-burner scrapes and indexes metrics timeseries.</p> <p>The reporting profile is very useful to reduce the number of documents sent to the configured indexer. Thanks to the combination of aggregations and instant queries for prometheus metrics, and 4 summaries for latency measurements, only a few documents will be indexed per benchmark. This flag makes possible to specify one or both of these profiles indistinctly.</p>"},{"location":"#customizing-workloads","title":"Customizing workloads","text":"<p>It is possible to customize any of the above workload configurations by extracting, updating, and finally running it:</p> <pre><code>$ kube-burner-ocp node-density --extract\n$ ls\nalerts.yml  metrics.yml  node-density.yml  pod.yml  metrics-report.yml\n$ vi node-density.yml                               # Perform modifications accordingly\n$ kube-burner-ocp node-density --pods-per-node=100  # Run workload\n</code></pre>"}]}