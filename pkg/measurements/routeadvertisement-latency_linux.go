// Copyright 2025 The Kube-burner Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package measurements

import (
	"context"
	"encoding/json"
	"fmt"
	"net"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	k8sconnector "github.com/cloud-bulldozer/go-commons/v2/k8s-connector"
	"github.com/kube-burner/kube-burner/pkg/config"
	"github.com/kube-burner/kube-burner/pkg/measurements"
	"github.com/kube-burner/kube-burner/pkg/measurements/metrics"
	"github.com/kube-burner/kube-burner/pkg/measurements/types"
	"github.com/kube-burner/kube-burner/pkg/util/fileutils"
	probing "github.com/prometheus-community/pro-bing"
	log "github.com/sirupsen/logrus"
	"github.com/vishvananda/netlink"
	"golang.org/x/sys/unix"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic/dynamicinformer"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/cache"
)

const (
	// Measurement name
	raLatencyMeasurement          = "raLatencyMeasurement"
	raLatencyQuantilesMeasurement = "raLatencyQuantilesMeasurement"
	// number of threads validating exported routes in export scenario
	exportWorkerCount = 20
	pingAttempts      = 100
	// number of threads generating external routes and validating them in import scenario
	importWorkerCount = 10
	// Tuning parameters for ping test
	importPingThreads             = 5
	importWaitBeforePingRetryMsec = 100
	importPingerTimeoutMsec       = 10
	exportWaitBeforePingRetryMsec = 100
	exportPingerTimeoutMsec       = 100
)

// default values for the job's input variables. This will be overridden with user passed values in the job template
// number of routes to be imported will be a multiple of numDummyIfaces and numAddressOnDummyIface
var numDummyIfaces = 20
var numAddressOnDummyIface = 10
var importRoutesCount = numDummyIfaces * numAddressOnDummyIface

// Max timeout to wait for finishing the import and export scenarios
var exportScenarioMaxTimeout time.Duration = 1 * time.Minute
var importScenarioMaxTimeout time.Duration = 1 * time.Minute

// Internal struct used to marshal PodAnnotation to the pod annotation
// pod IP address is derived from podAnnotation when the pod is created on cudn network
type podAnnotation struct {
	IPs      []string   `json:"ip_addresses"`
	MAC      string     `json:"mac_address"`
	Gateways []string   `json:"gateway_ips,omitempty"`
	Routes   []podRoute `json:"routes,omitempty"`

	IP      string `json:"ip_address,omitempty"`
	Gateway string `json:"gateway_ip,omitempty"`

	TunnelID int    `json:"tunnel_id,omitempty"`
	Role     string `json:"role,omitempty"`
}

// Internal struct used to marshal PodRoute to the pod annotation
type podRoute struct {
	Dest    string `json:"dest"`
	NextHop string `json:"nextHop"`
}

// routeImport type represents a route (using combination of "link" and "addr" fields) to be generated by the kube-burner during import scenario which will be imported into the cluster using bgp. "pods" member represent list of cudn pods this route should ping during ping test.
// Main thread will create "routeImport" variables and then write tp a channel. Import threads read from the channel in parallel, create routes and ping the provided cudn pods.
type routeImport struct {
	link string
	addr string
	pods []string
}

// channel for route advertisement informer
var stopCh = make(chan struct{})

type raMetric struct {
	Timestamp  time.Time `json:"timestamp"`
	MetricName string    `json:"metricName"`
	UUID       string    `json:"uuid"`
	JobName    string    `json:"jobName,omitempty"`
	// route advertisement name
	Name     string `json:"routeAdvertisementName"`
	Metadata any    `json:"metadata,omitempty"`
	// whether this metric represents route latency for import scenario or export scenario
	Scenario string `json:"scenario,omitempty"`
	// list of cudn advertised by this route advertisement
	cudn []string
	// when an ra exports multiple cudn subnets, we measure latecny for each cudn. So belowLatency slice is ping latency for each cudn.
	Latency []float64 `json:"latency,omitempty"`
	// ping test latency. Note: ReadyLatency is p99, calculated on Latency
	MinReadyLatency int `json:"minReadyLatency"`
	MaxReadyLatency int `json:"maxReadyLatency"`
	P99ReadyLatency int `json:"p99readyLatency"`
	// netlink route detection latency
	NetlinkRouteLatency    []float64 `json:"netlinkRouteLatency,omitempty"`
	MaxNetlinkRouteLatency int       `json:"maxNetlinkRouteLatency,omitempty"`
	MinNetlinkRouteLatency int       `json:"minNetlinkRouteLatency,omitempty"`
	P99NetlinkRouteLatency int       `json:"p99NetlinkRouteLatency,omitempty"`
}

// type holds cudn name and its pods
type cudnPods struct {
	cudn string
	pods []string
}

// type holds route detection and cund pod's ping timestamps
type netlinkRoutes struct {
	// linux doesn't allow adding duplicate routes, so routeTimestamp is not a slice
	// measure timestamp when a route (belonging to cudn's subnet) detected by the kernel
	routeTimestamp time.Time
	// cudn subnet is exported as route. KB detected this route in its host. Now KB pings the corresponding cudn's pod and stores success timestamp.
	pingTimestamps []time.Time
}

type raLatency struct {
	measurements.BaseMeasurement

	// list of cudn and their pods advertised by this route
	cudnSubnet map[string]cudnPods
	// timestamp when cudn is detected on external host and later events of ping tests
	cudnConnTimestamp sync.Map
	// how many routes verified during export scenario, helpful for closing the export threads
	verifiedExportRouteCount uint64
	// how many routes verified during impport scenario, helpful for closing the import threads
	verifiedImportRouteCount uint64
	// netlink channel we open with the kernel for route monitoring
	routeCh chan netlink.RouteUpdate
	// channel to notify closure of netlink monitoring and export workers
	exportDoneCh chan struct{}
	// channel to notify closure of import workers
	importDoneCh chan struct{}
	// channel wused between main thread and import threads. Main thread notifies import threads which routes to generate.
	routeImportChan chan routeImport
	wg              sync.WaitGroup
	connector       k8sconnector.K8SConnector
}

type raLatencyMeasurementFactory struct {
	measurements.BaseMeasurementFactory
}

var raGVR = schema.GroupVersionResource{
	Group:    "k8s.ovn.org",
	Version:  "v1",
	Resource: "routeadvertisements",
}

var cudnGVR = schema.GroupVersionResource{
	Group:    "k8s.ovn.org",
	Version:  "v1",
	Resource: "clusteruserdefinednetworks",
}

func NewRaLatencyMeasurementFactory(configSpec config.Spec, measurement types.Measurement, metadata map[string]any) (measurements.MeasurementFactory, error) {
	return raLatencyMeasurementFactory{
		measurements.NewBaseMeasurementFactory(configSpec, measurement, metadata),
	}, nil
}

func (plmf raLatencyMeasurementFactory) NewMeasurement(jobConfig *config.Job, clientSet kubernetes.Interface, restConfig *rest.Config, embedCfg *fileutils.EmbedConfiguration) measurements.Measurement {
	return &raLatency{
		BaseMeasurement: plmf.NewBaseLatency(jobConfig, clientSet, restConfig, raLatencyMeasurement, raLatencyQuantilesMeasurement, embedCfg),
	}
}

// unlike default pod network, when a pod is created on udn network, pod ip address is retrieved from pod annotations.
// we create list of cudn subnet and pod ip mappings. CUdn subnet is considered as a route exported to outside the cluster. When KB wants to ping test the cudn, it pings cudn's pods.
func (r *raLatency) getPods() error {
	var err error
	listOptions := metav1.ListOptions{LabelSelector: fmt.Sprintf("kube-burner-uuid=%s", r.Uuid)}
	nsList, err := r.ClientSet.CoreV1().Namespaces().List(context.TODO(), listOptions)
	if err != nil {
		log.Errorf("Error listing namespaces: %v", err)
		return err
	}
	for _, ns := range nsList.Items {
		podList, err := r.ClientSet.CoreV1().Pods(ns.Name).List(context.TODO(), listOptions)
		if err != nil {
			log.Errorf("Error listing pods in namespace %s: %v", ns.Name, err)
			return err
		}
		for _, pod := range podList.Items {
			podNetworks := make(map[string]podAnnotation)
			ovnAnnotation, ok := pod.Annotations["k8s.ovn.org/pod-networks"]
			if ok {
				if err := json.Unmarshal([]byte(ovnAnnotation), &podNetworks); err != nil {
					log.Errorf("failed to unmarshal ovn pod annotation  %v", err)
					continue
				}
				for pnet, val := range podNetworks {
					if pnet != "default" {
						var udn string
						parts := strings.Split(pnet, "/")
						if len(parts) == 2 {
							_, udn = parts[0], parts[1]
						} else {
							log.Debugf("Invalid input format")
							continue
						}
						ipAddr, subnet, err := net.ParseCIDR(val.IP)
						if err != nil {
							log.Debugf("Unable to get CIDR for IP")
							continue
						}
						subnetString := subnet.String()
						ipAddrString := ipAddr.String()
						cudnpods, exists := r.cudnSubnet[subnetString]
						if exists {
							cudnpods.pods = append(cudnpods.pods, ipAddrString)
							r.cudnSubnet[subnetString] = cudnpods
						} else {
							r.cudnSubnet[subnetString] = cudnPods{
								cudn: udn,
								pods: []string{ipAddrString},
							}
						}
					}
				}
			}
		}
	}
	return nil
}

// Record RouteAdvertisement name and creation timestamp when routeadvertisement resource is detected by the API
func (r *raLatency) handleAdd(obj any) {
	var cudn []string
	ra := obj.(*unstructured.Unstructured)
	networkSelectors, found, err := unstructured.NestedSlice(ra.UnstructuredContent(), "spec", "networkSelectors")
	if err != nil {
		log.Error(err)
		return
	}
	if !found || len(networkSelectors) == 0 {
		log.Errorf("No networkSelectors found")
		return
	}
	networkSelector, ok := networkSelectors[0].(map[string]interface{})
	if !ok {
		log.Errorf("NetworkSelector doesn't exist")
		return
	}
	labels, found, err := unstructured.NestedStringMap(networkSelector, "clusterUserDefinedNetworkSelector", "networkSelector", "matchLabels")
	if err != nil {
		log.Error(err)
		return
	}
	if !found {
		log.Errorf("No labels found in networkSelector")
		return
	}

	ls := &metav1.LabelSelector{}
	err = metav1.Convert_Map_string_To_string_To_v1_LabelSelector(&labels, ls, nil)
	if err != nil {
		log.Error(err)
		return
	}
	selector, err := metav1.LabelSelectorAsSelector(ls)
	if err != nil {
		log.Error(err)
		return
	}
	listOptions := metav1.ListOptions{}
	listOptions.LabelSelector = selector.String()
	udns, err := r.connector.DynamicClient().Resource(cudnGVR).Namespace(metav1.NamespaceAll).List(context.TODO(), listOptions)
	if err != nil {
		log.Error(err)
		return
	}
	for _, udn := range udns.Items {
		cname, _, _ := unstructured.NestedString(udn.UnstructuredContent(), "metadata", "name")
		cudn = append(cudn, cname)
	}
	raName, _, _ := unstructured.NestedString(ra.UnstructuredContent(), "metadata", "name")
	ts, _, _ := unstructured.NestedString(ra.UnstructuredContent(), "metadata", "creationTimestamp")
	t, err := time.Parse(time.RFC3339, ts)
	if err != nil {
		log.Error(err)
		return
	}
	log.Debugf("RA %s discovered at: %v created at: %v", raName, time.Now().UTC(), t.UTC())
	r.Metrics.LoadOrStore(raName, raMetric{
		Name:       raName,
		Timestamp:  t.UTC(),
		Latency:    []float64{},
		cudn:       cudn,
		MetricName: raLatencyMeasurement,
		UUID:       r.Uuid,
		Metadata:   r.Metadata,
		JobName:    r.JobConfig.Name,
		Scenario:   "ExportRoutes",
	})
}

// ping the given pod address
func pingAddress(srcIP, destIP string, pingerTimeoutMsec int) error {
	pinger, err := probing.NewPinger(destIP)
	if err != nil {
		log.Debugf("Failed to create pinger for %s: %v", destIP, err)
		return err
	}

	if srcIP != "" {
		// Required for raw sockets (root access needed)
		pinger.SetPrivileged(true)
		// Bind to the specified source IP
		pinger.Source = srcIP
	}
	pinger.Count = 1
	pinger.Timeout = time.Duration(pingerTimeoutMsec) * time.Millisecond
	if err := pinger.Run(); err != nil {
		log.Debugf("Ping to %s failed: %v", destIP, err)
		return err
	}
	return nil
}

/*
During the import scenario, ping pods of given list of cudns to validate if the cudn's gateway router imported the external routes are not.

Sequential ping testing introduces an artificial dependency where the latency measured for later pods is influenced by the execution time of previous tests.
So we are using go threads for parallel testing which allows for simultaneous latency measurement, providing a more accurate and independent assessment of each pod latency by preventing the latency of one pod from impacting the measurements of others.

we are using a semaphore importPingThreads to limit concurrent go threads to 5.
Also each ping will be attempted pingAttempts(set to 100) times with a sleep in between the attempts.

It records the timestamp when the ping is successful.
*/
func pingAllCudns(destCudnPods []string, srcAddr string) []float64 {
	importLatency := []float64{}
	var mutex sync.Mutex
	var wg sync.WaitGroup

	sem := make(chan struct{}, importPingThreads) // Semaphore to limit goroutines

	importTimestamp := time.Now().UTC()

	for _, destCudnPod := range destCudnPods {
		wg.Add(1)
		sem <- struct{}{} // Acquire a slot

		go func(destCudnPod string) {
			defer wg.Done()
			defer func() { <-sem }() // Release the slot when done

			pingSuccess := false
			for range pingAttempts {
				if err := pingAddress(srcAddr, destCudnPod, importPingerTimeoutMsec); err == nil {
					latency := float64(time.Since(importTimestamp).Milliseconds())

					mutex.Lock()
					importLatency = append(importLatency, latency)
					mutex.Unlock()

					pingSuccess = true
					break
				}
				time.Sleep(importWaitBeforePingRetryMsec * time.Millisecond)
			}

			if !pingSuccess {
				mutex.Lock()
				importLatency = append(importLatency, -1)
				mutex.Unlock()
			}
		}(destCudnPod)
	}

	wg.Wait() // Wait for all goroutines to finish
	return importLatency
}

/*
This import go thread, reads the provided ip address and interface from the channel, adds the ip address.
Kernel internally generates a route when we add the ip address.
BGP then advertises this route to the cluster and OVN add this into the cluster cudns.
This thread verifies if this route is properly advertised by BGP or not on the cluster cudns, by pinging each CUDN's pod.
The important thing here is using the newly added IP address as the source IP for the ping test
*/

func (r *raLatency) importWorker() {
	defer r.wg.Done()
	for {
		select {
		case mc, ok := <-r.routeImportChan:
			if !ok {
				return
			}
			// Get the netlink.Link object for the given interface name
			link, err := netlink.LinkByName(mc.link)
			if err != nil {
				log.Errorf("Failed to get link %s: %v", mc.link, err)
			}
			// Parse IP address
			addr, err := netlink.ParseAddr(mc.addr)
			if err != nil {
				log.Errorf("Failed to parse IP address: %v", err)
			}

			importTimestamp := time.Now().UTC()
			// Generate route by adding IP address to the interface
			if err := netlink.AddrAdd(link, addr); err != nil {
				log.Errorf("Failed to add IP address: %v", err)
			}
			ipAddr, _, err := net.ParseCIDR(mc.addr)
			if err != nil {
				log.Errorf("Failed to add IP address: %v", err)
			}
			importLatency := pingAllCudns(mc.pods, ipAddr.String())

			latencySummary := metrics.NewLatencySummary(importLatency, mc.addr)
			log.Tracef("%s: 50th: %d 95th: %d 99th: %d min: %d max: %d avg: %d\n", mc.addr, latencySummary.P50, latencySummary.P95, latencySummary.P99, latencySummary.Min, latencySummary.Max, latencySummary.Avg)

			m := raMetric{
				Name:            mc.addr,
				Timestamp:       importTimestamp,
				MetricName:      raLatencyMeasurement,
				Latency:         importLatency,
				MinReadyLatency: latencySummary.Min,
				MaxReadyLatency: latencySummary.Max,
				P99ReadyLatency: latencySummary.P99,
				UUID:            r.Uuid,
				Metadata:        r.Metadata,
				JobName:         r.JobConfig.Name,
				Scenario:        "ImportRoutes",
			}
			r.Metrics.LoadOrStore(mc.addr, m)
			atomic.AddUint64(&r.verifiedImportRouteCount, 1)

		case <-r.importDoneCh:
			return
		}
	}
}

/*
When KB creates RA CRDs, CUDN subnets are advertised. BGP's external frr adds these as linux routes.
Now kernel notifies KB about these routes using "routeCh" channel (which KB subscribed to earlier).

This export go thread, reads the route(nothing but the subnet), retrieve the corresponding cudn (remember we already created a map "r.cudnSubnet" of subnet, sudn and pods mapping during start of measurements code) pods and pings them.

Note: we have only one pod per cudn subnet. So when a cudn subnet is detected, we ping only one pod (i.e the subnet's corresponding pod). So we don't need parallel executing of ping test.
*/

func (r *raLatency) exportWorker() {
	r.wg.Done()
	for {
		select {
		case update, ok := <-r.routeCh:
			if !ok {
				return
			}
			if update.Type == unix.RTM_NEWROUTE {
				cudnpods, exists := r.cudnSubnet[update.Route.Dst.String()]
				if exists {
					// linux doesn't allow adding duplicate routes, so new routeTimestamp should be added
					log.Debugf("Netlink route: %s received for udn: %s at: %v", update.Route.Dst.String(), cudnpods.cudn, time.Now().UTC())
					val, _ := r.cudnConnTimestamp.LoadOrStore(cudnpods.cudn, netlinkRoutes{
						routeTimestamp: time.Now().UTC(),
						pingTimestamps: []time.Time{}})
					if nlRouteVal, ok := val.(netlinkRoutes); ok {
						pingSuccess := nlRouteVal.pingTimestamps
						for _, pod := range cudnpods.pods {
							for range pingAttempts {
								if err := pingAddress("", pod, exportPingerTimeoutMsec); err == nil {
									log.Debugf("Ping success to pod %s for the Netlink route: %s received for udn: %s at: %v", pod, update.Route.Dst.String(), cudnpods.cudn, time.Now().UTC())
									pingSuccess = append(pingSuccess, time.Now().UTC())
									break
								}
								time.Sleep(exportWaitBeforePingRetryMsec * time.Millisecond)
							}
						}
						nlRouteVal.pingTimestamps = pingSuccess
						atomic.AddUint64(&r.verifiedExportRouteCount, 1)
						r.cudnConnTimestamp.Store(cudnpods.cudn, nlRouteVal)
					}
				}
			}
		case <-r.exportDoneCh:
			break
		}
	}
}

// Import scenario creates dummy interfaces, we need to cleanup them
func (r *raLatency) deleteDummyInterface(i int) error {
	var err error
	ifaceName := fmt.Sprintf("dummy%d", i)
	link, err := netlink.LinkByName(ifaceName)
	if err != nil {
		return fmt.Errorf("failed to find interface %s: %v", ifaceName, err)
	}

	if err := netlink.LinkDel(link); err != nil {
		return fmt.Errorf("failed to delete interface %s: %v", ifaceName, err)
	}
	return err
}

/*
Import scenario creates dummy interfaces.

Note: Route is generated only when an ip address is added on the dummy interface.
*/
func (r *raLatency) createDummyInterface(i int) error {
	var err error
	// Define the name of the dummy interface
	ifaceName := fmt.Sprintf("dummy%d", i)

	// Create a dummy link (interface)
	dummy := &netlink.Dummy{
		LinkAttrs: netlink.LinkAttrs{
			Name: ifaceName,
		},
	}

	// Add the dummy interface
	if err := netlink.LinkAdd(dummy); err != nil {
		log.Errorf("Failed to add dummy interface: %v", err)
		return err
	}

	// Bring the interface up
	if err := netlink.LinkSetUp(dummy); err != nil {
		log.Errorf("Failed to bring up interface: %v", err)
		return err
	}

	// Verify interface exists
	_, err = netlink.LinkByName(ifaceName)
	if err != nil {
		log.Errorf("Failed to get interface: %v", err)
	}
	return err
}

/*
In the import scenario, main thread creates all the dummy interfaces. It then writes to the channel
1. which ip address should be created on these dummy interfaces and
2. list of all cudn pods to ping for the route import verification

Then it starts all import threads. All import threads simultaneously read from the channel and
1. adds ip address (kernel automatically generates the route when an ip address is added)
2. ping cudn pods (to verify if the above generated route is exchanged with cudn or not)
*/
func (r *raLatency) startImportScenario() error {
	var err error
	podsToPingDuingImport := []string{}
	for _, cpods := range r.cudnSubnet {
		podsToPingDuingImport = append(podsToPingDuingImport, cpods.pods[0])
	}
	if len(podsToPingDuingImport) == 0 {
		return nil
	}
	for i := range numDummyIfaces {
		err = r.createDummyInterface(i)
		if err != nil {
			return err
		}
	}
	importRoutesCount = numAddressOnDummyIface * numDummyIfaces
	r.routeImportChan = make(chan routeImport, importRoutesCount)
	for i := range numAddressOnDummyIface {
		for j := range numDummyIfaces {
			mm := routeImport{
				link: fmt.Sprintf("dummy%d", j),
				addr: fmt.Sprintf("20.%d.%d.1/24", j, i+1),
				pods: podsToPingDuingImport,
			}
			r.routeImportChan <- mm
		}
	}
	close(r.routeImportChan)
	// Start import worker goroutines
	for range importWorkerCount {
		r.wg.Add(1)
		go r.importWorker()
	}
	return nil
}

/*
Start export scenario
1. Main thread subscribes (through routeCh channel) from netlink route monitoring with the kernel
2. Starts export workers, which read from the subscribed routeCh channel
3. register an informer for router advertisement resource creation events
*/
func (r *raLatency) startExportScenario() error {
	var err error
	r.cudnConnTimestamp = sync.Map{}
	r.routeCh = make(chan netlink.RouteUpdate, 10000)

	if err = netlink.RouteSubscribe(r.routeCh, r.exportDoneCh); err != nil {
		log.Errorf("Failed to subscribe to route updates: %v", err)
		return err
	}

	// Start export worker goroutines
	for range exportWorkerCount {
		r.wg.Add(1)
		go r.exportWorker()
	}
	log.Infof("Creating Router Advertisement latency watcher for %s", r.JobConfig.Name)
	connector, err := k8sconnector.NewK8SConnector(r.RestConfig)
	if err != nil {
		log.Error(err)
		return err
	}
	r.connector = connector
	raFactory := dynamicinformer.NewFilteredDynamicSharedInformerFactory(r.connector.DynamicClient(), time.Minute, metav1.NamespaceAll, nil)
	raInformer := raFactory.ForResource(raGVR).Informer()
	raInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: r.handleAdd,
	})
	raFactory.Start(stopCh)
	raFactory.WaitForCacheSync(stopCh)
	return nil
}

// Read input variables from job templates
func (r *raLatency) setInputVars() {
	var err error
	for _, obj := range r.JobConfig.Objects {
		if val, ok := obj.InputVars["numDummyIfaces"]; ok {
			numDummyIfaces = val.(int)
		}
		if val, ok := obj.InputVars["numAddressOnDummyIface"]; ok {
			numAddressOnDummyIface = val.(int)
		}
		if val, ok := obj.InputVars["exportScenarioMaxTimeout"]; ok {
			exportScenarioMaxTimeout, err = time.ParseDuration(val.(string))
			if err != nil {
				log.Errorf("Failure parsing exportScenarioMaxTimeout: %v", err)
			}
		}
		if val, ok := obj.InputVars["importScenarioMaxTimeout"]; ok {
			importScenarioMaxTimeout, err = time.ParseDuration(val.(string))
			if err != nil {
				log.Errorf("Failure parsing importScenarioMaxTimeout: %v", err)
			}
		}
	}
}

// start raLatency measurement
func (r *raLatency) Start(measurementWg *sync.WaitGroup) error {
	// Reset latency slices, required in multi-job benchmarks
	var err error
	r.LatencyQuantiles, r.NormLatencies = nil, nil
	r.Metrics = sync.Map{}

	defer measurementWg.Done()

	if r.JobConfig.SkipIndexing {
		return nil
	}
	r.setInputVars()

	// channel to notify export workers to exit
	r.exportDoneCh = make(chan struct{})

	// cudn pods which will be pinged during both import and export scenarios
	r.cudnSubnet = make(map[string]cudnPods)

	// Maintain a list of cudn subnets and their pods, which will be used in both export and import scenarios
	r.getPods()

	if err = r.startExportScenario(); err != nil {
		return err
	}
	return nil
}

func (r *raLatency) Collect(measurementWg *sync.WaitGroup) {
	defer measurementWg.Done()
}

/*
Wait till the given export or import scenario completes.
Periodically check if the scenario completed by measuring how many routes validated.
Exit if we hit maxTimeout though the scenario might not have completed. We later notify all scenario threads to return in this case.
*/
func (r *raLatency) waitForSceanrioCompletion(desiredCount uint64, maxTimeout time.Duration, scenario string) {
	var count uint64
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()

	timeoutTimer := time.NewTimer(maxTimeout)
	defer timeoutTimer.Stop()

	for {
		select {
		case <-ticker.C:
			if scenario == "export" {
				count = atomic.LoadUint64(&r.verifiedExportRouteCount)
			} else {
				count = atomic.LoadUint64(&r.verifiedImportRouteCount)
			}
			log.Debugf("count %v , desiredCount %v", count, desiredCount)
			if count >= desiredCount {
				log.Debugf("Desired count reached, signaling stop.")
				// Give additional 10 seconds for threads to finish (ping test after detecting routes)
				time.Sleep(10 * time.Second)
				return
			}
		case <-timeoutTimer.C:
			log.Debugf("Timeout reached, signaling stop.")
			return
		}
	}
}

// Stop stops raLatency measurement
func (r *raLatency) Stop() error {
	var err error
	if r.JobConfig.SkipIndexing {
		return nil
	}

	// Wait till all CUDNs exported using RAs i.e wait for export scenario validation
	// We are assuming all CUDN's will be exported using RAs
	desiredCount := uint64(len(r.cudnSubnet))
	r.waitForSceanrioCompletion(desiredCount, exportScenarioMaxTimeout, "export")
	// stop export workers
	close(r.exportDoneCh)

	// start import scenario
	r.importDoneCh = make(chan struct{})
	if err = r.startImportScenario(); err != nil {
		return err
	}
	// wait for import scenario completion
	desiredCount = uint64(importRoutesCount)
	r.waitForSceanrioCompletion(desiredCount, importScenarioMaxTimeout, "import")
	// stop import workers
	close(r.importDoneCh)

	r.wg.Wait()

	// cleanup dummy interfaces
	for i := range numDummyIfaces {
		err = r.deleteDummyInterface(i)
		if err != nil {
			log.Error("Error deleting dummy interfaces: %w", err)
		}
	}
	return r.StopMeasurement(r.normalizeMetrics, r.getLatency)
}

func (r *raLatency) normalizeMetrics() float64 {
	r.Metrics.Range(func(key, value any) bool {
		m := value.(raMetric)
		if m.Scenario == "ExportRoutes" {
			for _, udn := range m.cudn {
				val, exists := r.cudnConnTimestamp.Load(udn)
				if exists {
					nlRouteVal := val.(netlinkRoutes)
					for _, ts := range nlRouteVal.pingTimestamps {
						m.Latency = append(m.Latency, float64(ts.Sub(m.Timestamp).Milliseconds()))
					}
					m.NetlinkRouteLatency = append(m.NetlinkRouteLatency, float64(nlRouteVal.routeTimestamp.Sub(m.Timestamp).Milliseconds()))
				}
			}
			// Index ping latency
			latencySummary := metrics.NewLatencySummary(m.Latency, m.Name)
			log.Tracef("%s: 50th: %d 95th: %d 99th: %d min: %d max: %d avg: %d\n", m.Name, latencySummary.P50, latencySummary.P95, latencySummary.P99, latencySummary.Min, latencySummary.Max, latencySummary.Avg)

			m.MinReadyLatency = latencySummary.Min
			m.MaxReadyLatency = latencySummary.Max
			m.P99ReadyLatency = latencySummary.P99

			// Index netlink route detection latency
			nrLatencySummary := metrics.NewLatencySummary(m.NetlinkRouteLatency, m.Name)
			log.Tracef("%s: 50th: %d 95th: %d 99th: %d min: %d max: %d avg: %d\n", m.Name, nrLatencySummary.P50, nrLatencySummary.P95, nrLatencySummary.P99, nrLatencySummary.Min, nrLatencySummary.Max, nrLatencySummary.Avg)

			m.MinNetlinkRouteLatency = nrLatencySummary.Min
			m.MaxNetlinkRouteLatency = nrLatencySummary.Max
			m.P99NetlinkRouteLatency = nrLatencySummary.P99
		}

		r.NormLatencies = append(r.NormLatencies, m)
		return true
	})
	return 0
}

func (r *raLatency) getLatency(normLatency any) map[string]float64 {
	raMetric := normLatency.(raMetric)
	return map[string]float64{
		"MinReadyLatency":        float64(raMetric.MinReadyLatency),
		"MaxReadyLatency":        float64(raMetric.MaxReadyLatency),
		"P99ReadyLatency":        float64(raMetric.P99ReadyLatency),
		"MinNetlinkRouteLatency": float64(raMetric.MinNetlinkRouteLatency),
		"MaxNetlinkRouteLatency": float64(raMetric.MaxNetlinkRouteLatency),
		"P99NetlinkRouteLatency": float64(raMetric.P99NetlinkRouteLatency),
	}
}
